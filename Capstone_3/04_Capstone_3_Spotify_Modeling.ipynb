{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Preprocessing <br />\n",
    "In this notebook we will be taking our now analyzed data and preparing it for modeling (which will occur in the next notebook - Regression Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import os.path\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('capstone3_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are dropping our non-numerical/categorical features for our feature. We could theoretically implement some dummy features (particularly for feature artist), but I determined it would simply be unnecessary as our end user would be Illenium anyways, The only way that it could be helpful is if he were to collaborate wth other popular artists in the future, and they would still need to determine what aspects of a song they would want to incorporate anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation = df.drop(columns=[\"artists\",\"id\",\"name\",\"feature_artist\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target feature will be popularity (how can we maximize it?), the other features will determine how best to maximize popularity so they will be the features that we compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = preparation.popularity\n",
    "features = preparation.drop(columns=[\"popularity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling our non-popularity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and testing data in preparation for modeling and the conclusion of the project. As stated above, we will be implementing regression models to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Modeling <br />\n",
    "\n",
    "Here we will test some of the most popular regression machine learning models. first we import all of the models modules and train them with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "r = Ridge().fit(X_train, y_train)\n",
    "l = Lasso().fit(X_train, y_train)\n",
    "rf = RandomForestRegressor().fit(X_train, y_train)\n",
    "knn = KNeighborsRegressor().fit(X_train, y_train)\n",
    "sv = SVR().fit(X_train, y_train)\n",
    "nn = MLPRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check the score, predictions, r^2 score, explained variance score, and mean absolute error for all of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression Score: 0.19140385952675698\n",
      "\n",
      " predictions: [52.36067904 46.12238296 47.8112182  55.52120398 49.59117454 54.24344913\n",
      " 47.32301572 49.20127545 45.57957551 47.8426646  51.37704596 51.73906102\n",
      " 50.1519536  49.32706049 49.65194156 47.53693472 46.78374365 51.32005343\n",
      " 50.93056483 44.15446007 56.51506345 47.74462297 44.57780843 48.20795386\n",
      " 57.87092072 53.47258141 53.11702266 43.93373503 45.38026224 48.55912001\n",
      " 51.00801367 44.89865745 50.01800176 50.69294819 51.19260663 50.16459423\n",
      " 52.88750595 49.02246016 48.58303064 49.64889352 46.17943798] \n",
      "\n",
      "\t r2_score : 0.013541704007281274\n",
      "\t explained_variance_score : 0.013782877228005752\n",
      "\t mean_absolute_error : 6.129081903733724\n",
      "----------------------------------\n",
      "Ridge Score: 0.19139185258337932\n",
      "\n",
      " predictions: [52.34361901 46.15505061 47.73489254 55.48326273 49.60136684 54.21573216\n",
      " 47.32681931 49.19396712 45.62613978 47.80911602 51.35394376 51.73629579\n",
      " 50.14643236 49.31676498 49.6692289  47.55194373 46.81611212 51.29946775\n",
      " 50.92155847 44.19283154 56.48318048 47.77025938 44.60698027 48.209501\n",
      " 57.62668901 53.44492026 53.08889998 43.96861499 45.41828382 48.55672958\n",
      " 51.01072378 44.93748271 50.04904941 50.69274514 51.18229602 50.13531772\n",
      " 52.86935527 49.03059945 48.60391389 49.64490658 46.20743765] \n",
      "\n",
      "\t r2_score : 0.01391598070482436\n",
      "\t explained_variance_score : 0.014179165501589153\n",
      "\t mean_absolute_error : 6.1338248690553545\n",
      "----------------------------------\n",
      "Lasso Score: 0.13468684789460306\n",
      "\n",
      " predictions: [51.32438982 47.83544749 46.64689379 53.10615344 49.84126521 52.31662944\n",
      " 47.25930835 49.27589513 47.67540884 48.34757116 49.91604972 51.14301269\n",
      " 49.94805745 49.45727226 51.76182879 50.70547391 49.84126521 51.17492061\n",
      " 49.98006518 47.36600079 54.04494703 47.97958208 47.51537019 49.48927999\n",
      " 49.7880188  52.07123685 51.34572831 47.53670868 47.50470095 48.47560208\n",
      " 51.1962591  48.81711767 50.22535797 49.57463394 50.6522275  49.89461143\n",
      " 51.54834412 49.89461143 49.4679415  48.46930034 48.59296376] \n",
      "\n",
      "\t r2_score : -0.004124354646640516\n",
      "\t explained_variance_score : -0.004081374895124279\n",
      "\t mean_absolute_error : 6.2388146491046905\n",
      "----------------------------------\n",
      "RandomForestRegressor Score: 0.8598416013970707\n",
      "\n",
      " predictions: [54.66 47.25 49.25 53.34 49.9  56.03 49.61 48.91 46.65 49.99 49.08 50.28\n",
      " 49.95 48.73 60.81 50.71 49.29 56.43 49.85 48.06 53.96 47.75 46.98 48.65\n",
      " 46.91 54.48 50.89 44.83 46.65 49.32 54.69 48.41 51.89 50.89 51.82 55.79\n",
      " 53.1  56.06 46.34 46.83 47.63] \n",
      "\n",
      "\t r2_score : -0.21933773490326547\n",
      "\t explained_variance_score : -0.2057829632473709\n",
      "\t mean_absolute_error : 6.877317073170732\n",
      "----------------------------------\n",
      "KNeighborsRegressor Score: 0.20483526366378935\n",
      "\n",
      " predictions: [49.6 48.4 46.  55.8 46.8 53.2 51.2 57.  45.8 52.4 57.  52.4 56.6 46.\n",
      " 52.2 49.2 44.2 48.4 42.8 45.  56.4 48.  45.8 50.  44.8 50.  47.6 47.\n",
      " 47.2 48.8 48.8 50.8 47.8 49.4 53.8 46.  48.2 46.4 51.8 49.6 46.2] \n",
      "\n",
      "\t r2_score : -0.06379087476802958\n",
      "\t explained_variance_score : -0.06209765150060775\n",
      "\t mean_absolute_error : 6.170731707317073\n",
      "----------------------------------\n",
      "SVR Score: 0.1475232938610439\n",
      "\n",
      " predictions: [49.27170233 47.56370193 48.92505859 51.28599423 49.13654792 50.95682466\n",
      " 47.72655086 48.98705657 47.27255834 49.71599261 49.5263765  50.19630038\n",
      " 48.5396534  48.11642289 50.44098505 49.60422249 48.92388315 49.66290292\n",
      " 48.37923401 46.904913   50.93852378 47.65449481 47.74370989 48.97441829\n",
      " 49.63309837 49.97689384 48.99451172 48.41200079 48.72003292 48.81436712\n",
      " 50.79028215 49.80683979 50.50382564 48.98269715 49.31086755 48.74454584\n",
      " 51.03407549 48.25447789 48.38095197 47.52899678 48.45435333] \n",
      "\n",
      "\t r2_score : -0.004239735143258816\n",
      "\t explained_variance_score : 0.0020110177440711974\n",
      "\t mean_absolute_error : 6.239641228463357\n",
      "----------------------------------\n",
      "MLPRegressor Score: -21.01610386598894\n",
      "\n",
      " predictions: [11.15467185 12.35945512  5.22639437 14.67316579 11.97286671 13.59227711\n",
      "  9.83982512 12.25525689  7.52635092 14.94488895 11.5255345  12.64766867\n",
      "  9.14178022 12.36296275 25.29112475 16.60795007 12.73896853 12.69241482\n",
      "  9.06983525  9.24495672 22.32926948 10.74526682 10.53914474 21.17333292\n",
      "  1.41959863 11.8247797  14.77877878 15.17408367 15.8255117  11.58827548\n",
      " 11.40499945 17.30772029 16.54344658  9.31128229 17.61746646 11.57339755\n",
      " 14.31148883  9.23402362  9.35706588  9.46734933 12.63305722] \n",
      "\n",
      "\t r2_score : -24.942463073677974\n",
      "\t explained_variance_score : -0.36704938443299384\n",
      "\t mean_absolute_error : 37.02371491343177\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "models = [lr, r, l, rf, knn, sv, nn]\n",
    "metrics = {\"r2_score\": metrics.r2_score, \"explained_variance_score\": metrics.explained_variance_score, \"mean_absolute_error\":metrics.mean_absolute_error}\n",
    "\n",
    "for model in models:\n",
    "    print(str(model).strip(\"()\"), 'Score:', str(model.score(X_train,y_train)))\n",
    "    print(\"\\n predictions:\", model.predict(X_test),\"\\n\")\n",
    "    for k, metric in metrics.items():\n",
    "        print(\"\\t\", k, \":\", metric(y_test, model.predict(X_test)))\n",
    "    print('-'*34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the Random Forest performs the best. This will be the best model to use for our general conclusions and moving forward with the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
